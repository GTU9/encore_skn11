{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 앙상블 (Ensemble)\n",
    "- 다양한 모델을 결합하여 예측 향상시키는 방법\n",
    "- 투표(Voting), 배깅(Bagging), 부스팅(Boosting), 스태킹(Staking) 네가지로 구분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting\n",
    "- hard voting : 여러 개의 예측치에 대해 다수결로 결정\n",
    "- soft voting : 여러 개의 예측 확률을 평균내어 결정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 위스콘신 유방암 데이터셋 (Wisconsin Breast Cancer Dataset)\n",
    "\n",
    "유방암의 악성(Malignant)과 양성(Benign)을 분류하기 위해 자주 사용되는 데이터셋\n",
    "(의학적인 이미지를 바탕으로 유방암 종양의 특징을 수치화한 데이터)\n",
    "\n",
    "**데이터셋 개요**\n",
    "- **목적**: 유방암 종양이 악성(Malignant)인지, 양성(Benign)인지 분류\n",
    "- **샘플 수**: 569개\n",
    "- **특징(Features) 수**: 30개\n",
    "- **타겟(Target)**: 0(악성) 또는 1(양성)\n",
    "\n",
    "**데이터 구성**\n",
    "1. **Radius mean**: 종양의 평균 반지름\n",
    "2. **Texture mean**: 종양의 표면의 거칠기\n",
    "3. **Perimeter mean**: 종양의 평균 둘레 길이\n",
    "4. **Area mean**: 종양의 평균 면적\n",
    "5. **Smoothness mean**: 종양의 매끄러움 정도\n",
    "6. **Compactness mean**: 종양의 압축도\n",
    "7. **Concavity mean**: 종양의 오목함\n",
    "8. **Concave points mean**: 종양의 오목한 점 개수\n",
    "9. **Symmetry mean**: 종양의 대칭성\n",
    "10. **Fractal dimension mean**: 종양의 프랙탈 차원 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 569\n",
      "\n",
      ":Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      ":Attribute Information:\n",
      "    - radius (mean of distances from center to points on the perimeter)\n",
      "    - texture (standard deviation of gray-scale values)\n",
      "    - perimeter\n",
      "    - area\n",
      "    - smoothness (local variation in radius lengths)\n",
      "    - compactness (perimeter^2 / area - 1.0)\n",
      "    - concavity (severity of concave portions of the contour)\n",
      "    - concave points (number of concave portions of the contour)\n",
      "    - symmetry\n",
      "    - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "    The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "    worst/largest values) of these features were computed for each image,\n",
      "    resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
      "    10 is Radius SE, field 20 is Worst Radius.\n",
      "\n",
      "    - class:\n",
      "            - WDBC-Malignant\n",
      "            - WDBC-Benign\n",
      "\n",
      ":Summary Statistics:\n",
      "\n",
      "===================================== ====== ======\n",
      "                                        Min    Max\n",
      "===================================== ====== ======\n",
      "radius (mean):                        6.981  28.11\n",
      "texture (mean):                       9.71   39.28\n",
      "perimeter (mean):                     43.79  188.5\n",
      "area (mean):                          143.5  2501.0\n",
      "smoothness (mean):                    0.053  0.163\n",
      "compactness (mean):                   0.019  0.345\n",
      "concavity (mean):                     0.0    0.427\n",
      "concave points (mean):                0.0    0.201\n",
      "symmetry (mean):                      0.106  0.304\n",
      "fractal dimension (mean):             0.05   0.097\n",
      "radius (standard error):              0.112  2.873\n",
      "texture (standard error):             0.36   4.885\n",
      "perimeter (standard error):           0.757  21.98\n",
      "area (standard error):                6.802  542.2\n",
      "smoothness (standard error):          0.002  0.031\n",
      "compactness (standard error):         0.002  0.135\n",
      "concavity (standard error):           0.0    0.396\n",
      "concave points (standard error):      0.0    0.053\n",
      "symmetry (standard error):            0.008  0.079\n",
      "fractal dimension (standard error):   0.001  0.03\n",
      "radius (worst):                       7.93   36.04\n",
      "texture (worst):                      12.02  49.54\n",
      "perimeter (worst):                    50.41  251.2\n",
      "area (worst):                         185.2  4254.0\n",
      "smoothness (worst):                   0.071  0.223\n",
      "compactness (worst):                  0.027  1.058\n",
      "concavity (worst):                    0.0    1.252\n",
      "concave points (worst):               0.0    0.291\n",
      "symmetry (worst):                     0.156  0.664\n",
      "fractal dimension (worst):            0.055  0.208\n",
      "===================================== ====== ======\n",
      "\n",
      ":Missing Attribute Values: None\n",
      "\n",
      ":Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      ":Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      ":Donor: Nick Street\n",
      "\n",
      ":Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. dropdown:: References\n",
      "\n",
      "  - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction\n",
      "    for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on\n",
      "    Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "    San Jose, CA, 1993.\n",
      "  - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and\n",
      "    prognosis via linear programming. Operations Research, 43(4), pages 570-577,\n",
      "    July-August 1995.\n",
      "  - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "    to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994)\n",
      "    163-171.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data=load_breast_cancer()\n",
    "print(data.DESCR)\n",
    "\n",
    "df=pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['traget']=data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 31 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   mean radius              569 non-null    float64\n",
      " 1   mean texture             569 non-null    float64\n",
      " 2   mean perimeter           569 non-null    float64\n",
      " 3   mean area                569 non-null    float64\n",
      " 4   mean smoothness          569 non-null    float64\n",
      " 5   mean compactness         569 non-null    float64\n",
      " 6   mean concavity           569 non-null    float64\n",
      " 7   mean concave points      569 non-null    float64\n",
      " 8   mean symmetry            569 non-null    float64\n",
      " 9   mean fractal dimension   569 non-null    float64\n",
      " 10  radius error             569 non-null    float64\n",
      " 11  texture error            569 non-null    float64\n",
      " 12  perimeter error          569 non-null    float64\n",
      " 13  area error               569 non-null    float64\n",
      " 14  smoothness error         569 non-null    float64\n",
      " 15  compactness error        569 non-null    float64\n",
      " 16  concavity error          569 non-null    float64\n",
      " 17  concave points error     569 non-null    float64\n",
      " 18  symmetry error           569 non-null    float64\n",
      " 19  fractal dimension error  569 non-null    float64\n",
      " 20  worst radius             569 non-null    float64\n",
      " 21  worst texture            569 non-null    float64\n",
      " 22  worst perimeter          569 non-null    float64\n",
      " 23  worst area               569 non-null    float64\n",
      " 24  worst smoothness         569 non-null    float64\n",
      " 25  worst compactness        569 non-null    float64\n",
      " 26  worst concavity          569 non-null    float64\n",
      " 27  worst concave points     569 non-null    float64\n",
      " 28  worst symmetry           569 non-null    float64\n",
      " 29  worst fractal dimension  569 non-null    float64\n",
      " 30  traget                   569 non-null    int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 137.9 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "traget\n",
       "1    357\n",
       "0    212\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['traget'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 준비 (분리)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X=data.data\n",
    "y=data.target\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### hard voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 점수 : 0.9647887323943662\n",
      "테스트 평가 점수 : 0.951048951048951\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "knn_clf=KNeighborsClassifier()\n",
    "lr_clf=LogisticRegression()\n",
    "dt_clf=DecisionTreeClassifier()\n",
    "\n",
    "voting_clf=VotingClassifier(\n",
    "    estimators=[\n",
    "        ('knn_clf',knn_clf),\n",
    "        ('lr_cff',lr_clf),\n",
    "        ('dt_clf',dt_clf)\n",
    "    ],\n",
    "    voting='hard'   # 기본값\n",
    ")\n",
    "\n",
    "# 앙상블 모델 학습\n",
    "voting_clf.fit(X_train,y_train)\n",
    "\n",
    "# 예측 및 평가\n",
    "y_pred_train=voting_clf.predict(X_train)\n",
    "acc_score_train=accuracy_score(y_train,y_pred_train)\n",
    "print('학습 점수 :',acc_score_train)\n",
    "\n",
    "y_pred_test=voting_clf.predict(X_test)\n",
    "acc_score_test=accuracy_score(y_test,y_pred_test)\n",
    "print('테스트 평가 점수 :',acc_score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "앙상블 예측값: [0 1 0 1 0 0 1 1 1 0]\n",
      "KNeighborsClassifier 개별 정확도 :  0.9371\n",
      "KNeighborsClassifier 예측값 : [0 1 0 1 0 0 1 1 1 0]\n",
      "LogisticRegression 개별 정확도 :  0.9441\n",
      "LogisticRegression 예측값 : [0 1 0 1 0 0 1 1 1 0]\n",
      "DecisionTreeClassifier 개별 정확도 :  0.8951\n",
      "DecisionTreeClassifier 예측값 : [1 1 0 1 0 0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# hard voting 작동 원리 == 다수결\n",
    "start, end = 40,50\n",
    "\n",
    "voting_clf_pred=voting_clf.predict(X_test[start:end])\n",
    "print('앙상블 예측값:' , voting_clf_pred)\n",
    "\n",
    "for classifier in [knn_clf, lr_clf, dt_clf]:\n",
    "    classifier.fit(X_train,y_train)\n",
    "    pred=classifier.predict(X_test)\n",
    "    acc_score=accuracy_score(y_test,pred)\n",
    "\n",
    "    class_name=classifier.__class__.__name__    # 클래스의 이름 속성\n",
    "    print(f'{class_name} 개별 정확도 : {acc_score: .4f}')\n",
    "    print(f'{class_name} 예측값 : {pred[start:end]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### soft voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 점수 : 0.9859154929577465\n",
      "테스트 평가 점수 : 0.9370629370629371\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "knn_clf=KNeighborsClassifier()\n",
    "lr_clf=LogisticRegression()\n",
    "dt_clf=DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "voting_clf=VotingClassifier(\n",
    "    estimators=[\n",
    "        ('knn_clf',knn_clf),\n",
    "        ('lr_cff',lr_clf),\n",
    "        ('dt_clf',dt_clf)\n",
    "    ],\n",
    "    voting='soft'   # 기본값\n",
    ")\n",
    "\n",
    "# 앙상블 모델 학습\n",
    "voting_clf.fit(X_train,y_train)\n",
    "\n",
    "# 예측 및 평가\n",
    "y_pred_train=voting_clf.predict(X_train)\n",
    "acc_score_train=accuracy_score(y_train,y_pred_train)\n",
    "print('학습 점수 :',acc_score_train)\n",
    "\n",
    "y_pred_test=voting_clf.predict(X_test)\n",
    "acc_score_test=accuracy_score(y_test,y_pred_test)\n",
    "print('테스트 평가 점수 :',acc_score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "앙상블 예측값: [[5.70263157e-01 4.29736843e-01]\n",
      " [1.08113730e-03 9.98918863e-01]\n",
      " [9.99622506e-01 3.77494355e-04]\n",
      " [3.35757426e-04 9.99664243e-01]\n",
      " [9.00993416e-01 9.90065841e-02]\n",
      " [1.00000000e+00 1.75163138e-13]\n",
      " [7.79971341e-05 9.99922003e-01]\n",
      " [1.83004552e-02 9.81699545e-01]\n",
      " [1.14568790e-03 9.98854312e-01]\n",
      " [9.32982089e-01 6.70179112e-02]]\n",
      "각 모델별 예측 값 평균:  [[5.70263157e-01 4.29736843e-01]\n",
      " [1.08113730e-03 9.98918863e-01]\n",
      " [9.99622506e-01 3.77494355e-04]\n",
      " [3.35757426e-04 9.99664243e-01]\n",
      " [9.00993416e-01 9.90065841e-02]\n",
      " [1.00000000e+00 1.75163138e-13]\n",
      " [7.79971341e-05 9.99922003e-01]\n",
      " [1.83004552e-02 9.81699545e-01]\n",
      " [1.14568790e-03 9.98854312e-01]\n",
      " [9.32982089e-01 6.70179112e-02]]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# soft voting 작동 원리 == 각 예측기의 확률값 평균\n",
    "\n",
    "start,end =40,50\n",
    "\n",
    "voting_clf_pred_proba=voting_clf.predict_proba(X_test[start:end])\n",
    "print('앙상블 예측값:' , voting_clf_pred_proba)\n",
    "\n",
    "averages= np.full_like(voting_clf_pred_proba,0)\n",
    "\n",
    "for classifier in [knn_clf, lr_clf, dt_clf]:\n",
    "    classifier.fit(X_train,y_train)\n",
    "    pred=classifier.predict(X_test)\n",
    "    acc_score=accuracy_score(y_test,pred)\n",
    "    pred_proba=classifier.predict_proba(X_test[start:end])\n",
    "\n",
    "    # 예측 확률 평균을 위한 합계\n",
    "    averages += pred_proba\n",
    "\n",
    "    class_name=classifier.__class__.__name__    # 클래스의 이름 속성\n",
    "    # print(f'{class_name} 개별 정확도 : {acc_score: .4f}')\n",
    "    # print(f'{class_name} 예측 확률 : {pred_proba}')\n",
    "\n",
    "# 예측 확률 평균 계산 및 출력\n",
    "calc_averages=averages/3\n",
    "print('각 모델별 예측 값 평균: ',calc_averages)\n",
    "print(np.array_equal(voting_clf_pred_proba,calc_averages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "- Bootstrap Aggregation\n",
    "- Bootstrap 방식의 샘플링 : 각 estimator 마다 훈련 데이터를 뽑을 때, 중복 값을 허용하는 방식\n",
    "- 분류 모델의 경우, 각 tree(estimator)의 예측값을 다수결(hard voting) 결정\n",
    "- 회귀 모델의 경우, 각 tree(estimator)의 예측값을 평균내어 결정\n",
    "- 기본적으로 100개의 tree 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**하이퍼 파라미터**\n",
    "| **하이퍼파라미터**      | **설명**                                                                                     | **기본값**      |\n",
    "|--------------------------|--------------------------------------------------------------------------------------------|-----------------|\n",
    "| `n_estimators`           | 생성할 트리의 개수 지정 (트리의 개수가 많을수록 성능이 좋아질 수 있지만 계산 비용 증가) | 100             |\n",
    "| `criterion`              | 분할 품질을 측정하는 기준 (분류에서는 \"gini\" 또는 \"entropy\"를 사용)                 | \"gini\"          |\n",
    "| `max_depth`              | 각 트리의 최대 깊이 (설정하지 않으면 트리는 잎 노드가 순수해질 때까지 계속 확장) | None            |\n",
    "| `min_samples_split`      | 내부 노드를 분할하기 위해 필요한 최소 샘플 수 (과적합 방지 목적)                   | 2               |\n",
    "| `min_samples_leaf`       | 잎 노드가 되기 위해 필요한 최소 샘플 수 (과적합 방지 목적)                          | 1               |\n",
    "| `max_features`           | 각 트리를 분할할 때 고려할 최대 특성 수 ()\"auto\", \"sqrt\", \"log2\" 중 선택하거나, 특정 숫자 지정 가능) | \"auto\"          |\n",
    "| `bootstrap`              | 각 트리를 만들 때 부트스트랩 샘플링을 사용할지 여부를 결정                               | True            |\n",
    "| `random_state`           | 결과의 재현성을 위해 난수 시드 고정                                                  | None            |\n",
    "| `n_jobs`                 | 병렬 계산을 위해 사용할 CPU 코어 수를 지정 (-1로 설정하면 모든 코어를 사용)           | None            |\n",
    "| `class_weight`           | 각 클래스의 가중치를 자동으로 계산하거나 직접 지정 가능 (불균형 데이터 처리에 유용)    | None            |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 점수:  1.0\n",
      "테스트 평가 점수:  0.972027972027972\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf=RandomForestClassifier(n_estimators=100,random_state=0)\n",
    "\n",
    "# 학습\n",
    "rf_clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred_train=rf_clf.predict(X_train)\n",
    "acc_score_train=accuracy_score(y_train,y_pred_train)\n",
    "print('학습 점수: ',acc_score_train)\n",
    "\n",
    "y_pred_test=rf_clf.predict(X_test)\n",
    "acc_score_test=accuracy_score(y_test,y_pred_test)\n",
    "print('테스트 평가 점수: ',acc_score_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426\n"
     ]
    }
   ],
   "source": [
    "# 100개의 DecisionTree 확인\n",
    "# print(rf_clf.estimators_)\n",
    "\n",
    "# 100개의 DecisionTree가 사용한 샘플데이터 확인\n",
    "print(len(rf_clf.estimators_samples_[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pystudy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
